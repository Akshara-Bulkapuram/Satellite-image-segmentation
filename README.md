# SATELLITE IMAGE SEGMENTATION

This project focuses on the semantic segmentation of satellite images of Mumbai, aiming to segment seven distinct land cover classes: Informal Settlements, Built-Up areas, Impervious Surfaces, Vegetation, Barren land, Water bodies, and
regions labeled as ’unknown’. for this purpose we implemented DeepLabv3+, MobileNet-V2-UNet, VGG16-UNet , Pix2pix GAN Models

## Dataset:

The satellite imagery of dataset of the Pleiades-1A satellite was used for this project. it consists of R, G, and B bands imagery dated 15th March 2017 covers an area of approximately 541.65 square kilometers of Greater Mumbai. the dataset consisits of 110 patches and each patches were
divided into tiles of size 128 x 128 making a total of 8910 tiles which is our dataset size. The images were labeled with six unique classes, namely: (1) vegetation; (2) built-up; (3) informal settlements; (4) impervious surfaces (roads/highways, streets, parking lots, road like area around buildings, etc.); (5) barren; and (6) water.
Below figures show the class color mapping for each class and satellite images along with their ground-truth masks.

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/425170cc-4cd9-48ee-a34d-d35647519666" width="256"   align="middle">

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/78a83af5-b233-4372-8e2e-9eebc48d83dd" width='500'   align="middle">




## Preprocessing

The dataset of size 8910 training images with 8910 training masks is divided into a train set consisting of 7128 training images and masks, a validation set consisting of 891 images and masks, and a test set consisting of 891 images and masks. As the first steps of preprocessing, we create a training data generator, validation data generator, and test data generator
which shuffle, crop, brighten, and process the image into desired dimensions(128 x 128) to be sent to model.fit method. we create a color map that maps rgb values of classes to numerical indices for each class. in the data generator, we utilise this color map to convert rgb representation (128x128x3) of the target mask to one hot representation(128x128x7) with
class labels when loading the satellite-image and mask as batches. we also define number of epochs to train models for as 50, batch size as 32, number of training steps per epoch 223 and validation steps as 28. we also define a function
that computes dice coefficient. 

## what is dice coefficient

dice coefficient is a statistic used to measure the similarity between two sets of pixels: the predicted segmentation mask (generated by a model) and the ground truth segmentation mask. The Dice coefficient is defined as the ratio of twice the intersection of the two sets (i.e., the number of pixels that are correctly classified as both
foreground in both the predicted and ground truth masks) to the sum of the sizes of the two sets (i.e., the total number of pixels classified as foreground in both masks). we use dice coefficient as a metric to evaluate and compare our models.

## Deeplabv3plus
The DeepLabv3+ architecture has an encoder-decoder framework with atrous separable convolutions, It has been widely used in semantic image segmentation tasks. It has following three important components
![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/a28108d5-8bf3-4c2c-a039-67ea85e1b639)

1. Encoder Module: - The encoder module, responsible
for capturing high-level semantic information, utilizes Aligned
Xception instead of ResNet-101 as its feature extractor. - Unlike traditional encoder-decoder networks, where max-pooling
operations are employed for down-sampling, DeepLabv3+
replaces these operations with depth-wise separable convolutions. This modification helps in preserving spatial information
while reducing computational overhead. - The encoder gradually reduces the spatial dimensions of feature maps while
capturing increasingly abstract features.

3. Decoder Module: - The decoder module aims to recover
spatial information lost during the encoding process and refine
segmentation masks. - DeepLabv3+ employs an output stride
of 16 in the encoder, resulting in a down-sampling factor of 16.
To up-sample the encoded features, bilinear interpolation with
a factor of 4 is initially applied. - The up-sampled features
are concatenated with corresponding low-level features from
the encoder module, which have the same spatial dimensions.
Before concatenation, 1x1 convolutions are used to reduce
the number of channels in the low-level features, facilitating
efficient feature fusion. - After concatenation, several 3x3
convolutions are applied to refine the feature representation. -
Finally, the features are up-sampled by a factor of 4 to match
the size of the input image, resulting in the final segmentation
mask.

3. Atrous Spatial Pyramid Pooling (ASPP): - ASPP is a
key component of DeepLabv3+ that facilitates multi-scale
feature aggregation and contextual reasoning. - It consists
of parallel atrous convolutional layers with different dilation
rates, allowing the network to capture contextual information
at multiple scales. - ASPP enhances the model’s ability to
segment objects of varying sizes and capture fine details.
The final output of the network is a pixel-wise segmentation
mask, where each pixel is assigned a class label corresponding
to the object or region it belongs to.

## UNet

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/474131b8-eae1-4ee2-8408-6c2f6204dc18)

The architecture of UNet is characteriuzed by a U-shaped structure, which consists of two main parts: the Encoder (contracting path) and the Decoder (expansive path). The Ushaped encoder-decoder network architecture is characterized
by four encoder blocks and four decoder blocks that are connected via a bridge. The encoder network (contracting path) half the spatial dimensions and double the number of filters
(feature channels) at each encoder block. The contracting path resembles a typical convolutional neural network with
successive convolutional and max-pooling layers, designed to
capture context and resuce spatial dimensions. The expansive path uses transposed convolutions to gradually upsample the
feature maps and recover the spatioal information lost during
the contracting path. Skip connections are employed between corresponding layers in the contracting and expansive paths to
help retain fine-grained details. Apart from this, UNet features
Skip Connections, which provide additional information that
helps the decoder to generate better semantic features. They also act as a shortcut connection that helps the indirect flow of gradients to the earlier layers without any degradation

## VGG16-UNet 

we used an UNet based ensemble model where VGG16 serves as the feature extractor and UNet's decoder upsamples feature maps to reconstruct spatial details.
VGG16 is known for its simplicity and effectiveness. It consists of 16 layers, including 13 convolutional layers and 3 fully connected layers. The convolutional layers use 3x3 filters with a stride of 1, and
they are followed by max-pooling layers with 2x2 filters and a stride of 2. The fully connected layers are followed by a softmax layer for classification UNet utilizes skip connections to preserve fine-grained details during upsampling.

## MobileNetV2-UNet

MobileNet-V2 is a lightweight CNN optimized for mobile and embedded devices, incorporating inverted residuals and depthwise separable convolutions.Similar to VGG16 UNet, MobileNet-V2 acts as the feature extractor while UNet's decoder facilitates precise segmentation by upsampling feature maps.
This implementation focuses on model size and efficiency without compromising accuracy, suitable for resource-constrained environments.

## Pix2pix GAN

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/464df648-e757-4ea1-9652-fa1775867eaf)

Pix2Pix is based on conditional generative adversarial networks (CGAN) to learn a mapping function that maps an
input image into an output image.Pix2Pix like GAN, CGAN
is also composed of two networks, the generator and the
discriminator.


1) The Generator Architecture: The purpose of the generator is to take an input image and convert it into the desired
image (output or footprint) by implementing the necessary
tasks. There are two types of generators, including the encoderdecoder and the U-Net. The final difference is to have skipped
connections.Encoder-decoder networks translate and compress
the input images into a low-dimensional vector presentation
(Bottleneck). Then the process is reversed and the multitude of
low-level information exchanged between the input and output
can be used to execute all the necessary information through
the network. To bypass the Bottleneck part of the information,
they added a hop connection between each layer i and n-i,
where i is the total number of layers. Note that the shape of
the generator with the hop connections looks like a U-Net as
demonstrated in Fig. As can be seen in the U-Net architecture,
information from earlier layers will be integrated into later
layers, and thanks to the use of hop connections, they require
no size changes or projections.


2) The Discriminator Architecture: The task of the discriminator is to measure the similarity of the input image with an
unknown image. This unknown image belongs to the dataset
(as a target image) or is an output image provided by the generator. The PatchGAN discriminator of the Pix2Pix network is
used as a single component to classify the individual patches
(N x N) of the image as real or fake. since the number of
parameters of the PatchGAN discriminator is very small, the
classification of the entire image is faster. The architecture of
the PatchGAN discriminator is shown in the Fig. The reason
this architecture is called ”PatchGAN” is that each pixel in
the output of the discriminator (30 × 30 image) corresponds
to the 70×70 patch in the input image. It is also worth noting
that since the size of the input images is 256×256, the patches
overlap considerably.


3) Training and loss functions: the Pix2Pix network
weights are adjusted in two steps:
In the first step, the discriminator (figure below) takes the
input (satellite image)/target (ground truths with overlapping
contours) and then input (satellite image)/output (generator
output) pairs, to estimate their realism. Then, the adjustment
of the discriminator weights is performed according to the
classification error of the mentioned pairs
Fig. 9. Pix2pix Generator
Fig. 10. Pix2pix discriminator
In the second step, the generator weights are adjusted using
the output of the discriminator and the difference between the
output images and the target images.
Gradually, with the help of an Adam optimization function,
the loss function learns to reduce the prediction error. to train
both generator and discriminator models we use seperate loss
functions for each. The adversarial loss is computed using
categorical crossentropy (CCE) loss. This loss compares the
output of the discriminator when applied to the generated
(fake) images with a tensor of ones (indicating real images).
The goal of the generator is to fool the discriminator into
classifying its generated images as real. The L1 loss measures
the pixel-wise difference between the generated output and
the target (ground truth) images. The L1 loss encourages the
generated images to be similar to the target images in terms
of pixel values. The total generator loss is computed as the
sum of the adversarial loss and a scaled L1 loss. The scaling
factor (‘LAMBDA‘) set to 0.1 in our case, controls the relative
importance of the adversarial loss compared to the L1 loss.
for the discriminator loss function we use binary cross
entropy as the job of the discriminator is a simple one to
classify if the generated segmented mask is real(same as target
mask) or fake

## Results
The following are the results we recieve on test using each of the four models

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/ea1e7aac-9c48-447f-ae6d-cfb4f63c4fe6)

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/9a934f52-bf93-47d0-bc82-2bb4d39b93d3)

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/ad2ff3d2-fd85-4929-872b-1896125fcfad)

![image](https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/1b9943de-9c92-4e1c-a7d4-26271771e5b2)

pixel classification accuracies for each model are 

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/2f96cbe9-b442-4c55-a4d8-e00f5b972f12" width="256"  align="middle">

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/24eb1c18-7cf1-432d-a7ea-8c6749f3225a" width="256"  align="middle">

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/07efd89b-83df-4db9-ba95-4a88a9f01d58" width="256"  align="middle">

<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/23b2a2d5-ec89-483c-8a6e-84394fdd9bfa" width="256"   align="middle">


## dice coefficient and overall comparision is as follows:


<img src="https://github.com/Akshara-Bulkapuram/Satellite-image-segmentation/assets/94600166/4a508ab6-3584-4ae3-b451-23d2e3e1c9d5" width="500"   align="middle">

we see that overall DeepLabV3+ has performed the best giving a dice coefficient of 0.9 followed by Pix2Pix GAN .. 



